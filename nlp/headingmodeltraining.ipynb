{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "60631ff4-bb9d-4d5d-8fa7-f134b3dd40b7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "arr = [\"zero\", \"one\", \"two\", \"three\", \"four\", \"five\", \"six\", \"seven\", \"eight\", \"nine\"]\n",
    "\n",
    "\n",
    "def fun(s):\n",
    "    s = s.lower().split()\n",
    "    ans = \"\"\n",
    "    for i in range(len(s)):\n",
    "        for j in range(10):\n",
    "            if(s[i].count(arr[j])>0):\n",
    "                ans += str(j)\n",
    "                break\n",
    "        if(len(ans)==3):\n",
    "            break\n",
    "    return ans\n",
    "\n",
    "from transformers import pipeline, AutoTokenizer, DefaultDataCollator, AutoModelForQuestionAnswering, TrainingArguments, Trainer\n",
    "import pandas as pd\n",
    "from datasets import Dataset\n",
    "\n",
    "file_path = '~/advanced/nlp.jsonl'\n",
    "data = pd.read_json(path_or_buf=file_path, lines=True,dtype={'key':'str', 'transcript':'str', 'tool':'str', 'heading':'str', 'targtet':'str'})\n",
    "\n",
    "model_name = \"deepset/roberta-base-squad2\"\n",
    "nlp = pipeline('question-answering', model=model_name, tokenizer=model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "c6800679-3928-46d9-97a1-65909bf54b04",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "524  300 Engage target\n",
      "716 0 095 zero\n",
      "722  140 air defense turrets\n",
      "730 30 130 three zero heading\n",
      "773 0 090 zero\n",
      "774  155 Deploy surface-to-air missiles\n",
      "1084  225 urgent message\n",
      "1505  010 EMP\n",
      "1683  165 target is a purple commercial aircraft\n",
      "1777  350 surface-to-air missiles\n",
      "2144  280 surface-to-air missiles\n",
      "2730 80 280 eight zero heading\n",
      "2898 0 095 zero\n",
      "3151 15 115 one five\n",
      "3487 10 310 one zero\n",
      "3500\n"
     ]
    }
   ],
   "source": [
    "data = data.rename(columns={'transcript': 'context'})\n",
    "data = data.drop(columns=['key', 'tool', 'target'])\n",
    "questions = [\"What is the heading\" for q in data[\"context\"]]\n",
    "data[\"question\"] = questions\n",
    "\n",
    "b = []\n",
    "r = []\n",
    "for i in range(len(data)):\n",
    "    QA_input = {\n",
    "        'question': 'What is the heading',\n",
    "        'context': data[i:i+1][\"context\"].item()\n",
    "    }\n",
    "    n = nlp(QA_input)['answer']\n",
    "    c = data[i:i+1][\"heading\"].item()\n",
    "    d = fun(n)\n",
    "    if(d != c):\n",
    "        r.append(i)\n",
    "        print(i, d, c, n)\n",
    "    else:\n",
    "        b.append({'answer_start': [data[i:i+1][\"context\"].item().find(n)], 'text': [n]})\n",
    "print(len(data))\n",
    "data = data.drop(r)\n",
    "data['answers']=b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "0b72190f-460a-4ca3-8e3a-73aec297b8dd",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "81a241c6974e4c51bc8435f911aa0557",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b640e829b9c442ffae35c6c7c3316d0c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/483 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "50cc4f744ee843149ec70074a39ba886",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "57f7588930784771ad707ff7eee85c78",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "data = Dataset.from_pandas(data)\n",
    "data = data.train_test_split(test_size=0.2)\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"distilbert/distilbert-base-uncased\")\n",
    "\n",
    "def preprocess_function(examples):\n",
    "    questions = [q.strip() for q in examples[\"question\"]]\n",
    "    inputs = tokenizer(\n",
    "        questions,\n",
    "        examples[\"context\"],\n",
    "        max_length=384,\n",
    "        truncation=\"only_second\",\n",
    "        return_offsets_mapping=True,\n",
    "        padding=\"max_length\",\n",
    "    )\n",
    "\n",
    "    offset_mapping = inputs.pop(\"offset_mapping\")\n",
    "    answers = examples[\"answers\"]\n",
    "    start_positions = []\n",
    "    end_positions = []\n",
    "\n",
    "    for i, offset in enumerate(offset_mapping):\n",
    "        answer = answers[i]\n",
    "        start_char = answer[\"answer_start\"][0]\n",
    "        end_char = answer[\"answer_start\"][0] + len(answer[\"text\"][0])\n",
    "        sequence_ids = inputs.sequence_ids(i)\n",
    "\n",
    "        # Find the start and end of the context\n",
    "        idx = 0\n",
    "        while sequence_ids[idx] != 1:\n",
    "            idx += 1\n",
    "        context_start = idx\n",
    "        while sequence_ids[idx] == 1:\n",
    "            idx += 1\n",
    "        context_end = idx - 1\n",
    "\n",
    "        # If the answer is not fully inside the context, label it (0, 0)\n",
    "        if offset[context_start][0] > end_char or offset[context_end][1] < start_char:\n",
    "            start_positions.append(0)\n",
    "            end_positions.append(0)\n",
    "        else:\n",
    "            # Otherwise it's the start and end token positions\n",
    "            idx = context_start\n",
    "            while idx <= context_end and offset[idx][0] <= start_char:\n",
    "                idx += 1\n",
    "            start_positions.append(idx - 1)\n",
    "\n",
    "            idx = context_end\n",
    "            while idx >= context_start and offset[idx][1] >= end_char:\n",
    "                idx -= 1\n",
    "            end_positions.append(idx + 1)\n",
    "\n",
    "    inputs[\"start_positions\"] = start_positions\n",
    "    inputs[\"end_positions\"] = end_positions\n",
    "    return inputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "524159b2-036b-47c5-9f2c-f8ccef83929a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7c0af80b6a784b4c9e28817b80e9ac2f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2788 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "472d0ad8ccff46d787bdd3e46f30dba0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/697 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2c07b55ab7de43dcb23e42687c0a3618",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/268M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForQuestionAnswering were not initialized from the model checkpoint at distilbert/distilbert-base-uncased and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='525' max='525' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [525/525 05:58, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.041266</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.036767</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.237600</td>\n",
       "      <td>0.036393</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenized_data = data.map(preprocess_function, batched=True, remove_columns=data[\"train\"].column_names)\n",
    "\n",
    "data_collator = DefaultDataCollator()\n",
    "\n",
    "model = AutoModelForQuestionAnswering.from_pretrained(\"distilbert/distilbert-base-uncased\")\n",
    "\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"headingmodel\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.01,\n",
    "\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_data[\"train\"],\n",
    "    eval_dataset=tokenized_data[\"test\"],\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "trainer.save_model('headingmodel')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "d5081624-0944-4a51-8146-328739574b1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "arr = [\"zero\", \"one\", \"two\", \"three\", \"four\", \"five\", \"six\", \"seven\", \"eight\", \"nine\"]\n",
    "\n",
    "\n",
    "file_path = 'nlp.jsonl'\n",
    "data = pd.read_json(path_or_buf=file_path, lines=True,dtype={'key':'str', 'transcript':'str', 'tool':'str', 'heading':'str', 'targtet':'str'})\n",
    "question_answerer = pipeline(\"question-answering\", model=\"headingmodel\")\n",
    "j = 0\n",
    "for i in range((len(data))):\n",
    "    question = \"What direction to deploy weapon in\"\n",
    "    context = data[i:i+1][\"transcript\"].item()\n",
    "\n",
    "    a = question_answerer(question=question, context=context)[\"answer\"]\n",
    "    b = data[i:i+1][\"heading\"].item()\n",
    "    c = fun(a)\n",
    "    if(c!=b):\n",
    "        j+=1\n",
    "        print(i,j,c,b,a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea93ee56-e4ba-42b5-bef1-72569136e5ac",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "conda-base-py",
   "name": "workbench-notebooks.m120",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/workbench-notebooks:m120"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
